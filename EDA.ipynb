{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "swiss-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import nltk\n",
    "# from wordcloud import WordCloud\n",
    "from nrclex import NRCLex\n",
    "\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-census",
   "metadata": {},
   "source": [
    "# Post Frequency by Day\n",
    "\n",
    "Note: DataFrame is NOT already sorted by date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "legal-clone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't know how many of you read it, but a fe...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The reason that it occurred to me that I may h...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes I don't really feel like a participa...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       time  score\n",
       "0  I don't know how many of you read it, but a fe...  01Jan2017      6\n",
       "1                                          [deleted]  01Jan2017     10\n",
       "2  The reason that it occurred to me that I may h...  01Jan2017      1\n",
       "3  Sometimes I don't really feel like a participa...  01Jan2017      5\n",
       "4                                          [deleted]  01Jan2017      3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw_data_2017/01Jan2017.csv').drop(['Unnamed: 0'], axis=1)\n",
    "for year in range(2017, 2021):\n",
    "    for file in os.listdir(f'raw_data_{year}'):\n",
    "        if file[-3:] == 'csv':\n",
    "            df_new = pd.read_csv(f'raw_data_{year}/{file}').drop(['Unnamed: 0'], axis=1)\n",
    "            df_new.time = df_new.time.apply(lambda x: file[:-4])\n",
    "            df = pd.concat([df, df_new]).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "exposed-austin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reformat dates\n",
    "df.time = df.time.apply(lambda x: datetime.strptime(x, '%d%b%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "answering-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "df = df.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "nervous-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequencies for each date\n",
    "sorted_dates = df.time.value_counts().sort_index()\n",
    "\n",
    "freq = pd.DataFrame(sorted_dates).reset_index().rename(columns={'time': 'num_posts', 'index': 'date'})\n",
    "\n",
    "# calculate SMOOTHED frequencies \n",
    "smoothed_freq = pd.DataFrame(sorted_dates).rolling(30).mean().reset_index().rename(columns={'time': 'num_posts',\n",
    "                                                                                            'index': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "supported-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = alt.Chart(freq).mark_line(opacity=0.3,\n",
    "                                  ).encode(x='date', y='num_posts'\n",
    "                                  ).properties(title='Frequency of r/MentalHealth Posts'\n",
    "                                  ).interactive(bind_y=False)\n",
    "\n",
    "smoothed = alt.Chart(smoothed_freq.reset_index()).mark_line(\n",
    "                                                           ).encode(x='date', y='num_posts', tooltip=['date', 'num_posts']\n",
    "                                                           ).properties(title='Frequency of r/MentalHealth Posts'\n",
    "                                                           ).interactive(bind_y=False)\n",
    "\n",
    "chart = (series + smoothed).properties(width=800, height=300)\n",
    "chart.save('figures/post_freq.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-guyana",
   "metadata": {},
   "source": [
    "# Emotion Content\n",
    "as measured by NRCLex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-kingdom",
   "metadata": {},
   "source": [
    "**ONLY RUN THESE CELLS ONCE ============================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "optional-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more EDA using emotion data package NRCLex\n",
    "#question: should i standardize each post by number of tokens and then standardize whole weeks count by number of posts?\n",
    "features = {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5,\n",
    "            'surprise': 6, 'trust': 7, 'negative': 8, 'positive': 9}\n",
    "\n",
    "dates = sorted([datetime.strftime(datetime.strptime(file[:-4], '%d%b%Y'), '%Y-%m-%d') \\\n",
    "                for year in range(2017, 2021) \\\n",
    "                for file in os.listdir(f'raw_data_{year}') \\\n",
    "                if file[-3:] == 'csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "conventional-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "df_text = df[df['text'] != '[deleted]']\n",
    "\n",
    "for date in dates:\n",
    "    data.append(df_text[df_text['time'] == datetime.strptime(date, '%Y-%m-%d')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "amber-fairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [08:07<00:00,  1.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>1.835547</td>\n",
       "      <td>2.238472</td>\n",
       "      <td>1.193852</td>\n",
       "      <td>2.313088</td>\n",
       "      <td>1.417699</td>\n",
       "      <td>2.208626</td>\n",
       "      <td>0.910312</td>\n",
       "      <td>2.029548</td>\n",
       "      <td>3.790479</td>\n",
       "      <td>3.521862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>1.835547</td>\n",
       "      <td>2.238472</td>\n",
       "      <td>1.193852</td>\n",
       "      <td>2.313088</td>\n",
       "      <td>1.417699</td>\n",
       "      <td>2.208626</td>\n",
       "      <td>0.910312</td>\n",
       "      <td>2.029548</td>\n",
       "      <td>3.790479</td>\n",
       "      <td>3.521862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>1.708164</td>\n",
       "      <td>1.939780</td>\n",
       "      <td>1.071222</td>\n",
       "      <td>2.388535</td>\n",
       "      <td>1.331789</td>\n",
       "      <td>2.547771</td>\n",
       "      <td>0.897510</td>\n",
       "      <td>2.012160</td>\n",
       "      <td>4.067748</td>\n",
       "      <td>3.126809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>1.708164</td>\n",
       "      <td>1.939780</td>\n",
       "      <td>1.071222</td>\n",
       "      <td>2.388535</td>\n",
       "      <td>1.331789</td>\n",
       "      <td>2.547771</td>\n",
       "      <td>0.897510</td>\n",
       "      <td>2.012160</td>\n",
       "      <td>4.067748</td>\n",
       "      <td>3.126809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>1.574550</td>\n",
       "      <td>2.313625</td>\n",
       "      <td>0.851542</td>\n",
       "      <td>1.911954</td>\n",
       "      <td>1.349614</td>\n",
       "      <td>2.056555</td>\n",
       "      <td>0.723008</td>\n",
       "      <td>2.313625</td>\n",
       "      <td>3.631105</td>\n",
       "      <td>3.213368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               anger  anticipation   disgust      fear       joy   sadness  \\\n",
       "2016-12-31  1.835547      2.238472  1.193852  2.313088  1.417699  2.208626   \n",
       "2017-01-01  1.835547      2.238472  1.193852  2.313088  1.417699  2.208626   \n",
       "2017-01-02  1.708164      1.939780  1.071222  2.388535  1.331789  2.547771   \n",
       "2017-01-03  1.708164      1.939780  1.071222  2.388535  1.331789  2.547771   \n",
       "2017-01-04  1.574550      2.313625  0.851542  1.911954  1.349614  2.056555   \n",
       "\n",
       "            surprise     trust  negative  positive  \n",
       "2016-12-31  0.910312  2.029548  3.790479  3.521862  \n",
       "2017-01-01  0.910312  2.029548  3.790479  3.521862  \n",
       "2017-01-02  0.897510  2.012160  4.067748  3.126809  \n",
       "2017-01-03  0.897510  2.012160  4.067748  3.126809  \n",
       "2017-01-04  0.723008  2.313625  3.631105  3.213368  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_by_day = np.zeros((len(data), 10))\n",
    "for i in tqdm(range(0, len(data))):\n",
    "    texts = data[i]['text'].values\n",
    "    num_tokens_per_day = 0\n",
    "    for j in texts:\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(j.lower())\n",
    "            num_tokens_per_day += len(tokens)\n",
    "            emotion = NRCLex(j.lower())\n",
    "            emotion_dict = emotion.raw_emotion_scores\n",
    "            for key in emotion_dict:\n",
    "                col = features[key]\n",
    "                count = emotion_dict[key]\n",
    "                emotion_by_day[i, col] += count\n",
    "        except:\n",
    "            # skip over non-text posts\n",
    "            continue\n",
    "        \n",
    "    #normalize frequency of each emotion (over the 1 week period) by number of tokens (over the 1 week period)\n",
    "    emotion_by_day[i, :] = (emotion_by_day[i, :] / num_tokens_per_day) * 100\n",
    "            \n",
    "emotion_scores = pd.DataFrame(emotion_by_day, \n",
    "                              columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
    "                                         'sadness', 'surprise', 'trust', 'negative', 'positive'], \n",
    "                              index = dates)\n",
    "emotion_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "gentle-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure date is in the right format\n",
    "emotion_scores_new = emotion_scores.reset_index().rename({'index': 'date'}, axis=1)\n",
    "emotion_scores_new.date = emotion_scores_new.date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "emotion_scores_new.to_json('clean_data/emotion_scores.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-accountability",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "answering-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rolling mean to reduce noise\n",
    "emotion_scores = pd.read_json('clean_data/emotion_scores.json')\n",
    "emotion_scores = pd.concat([emotion_scores.date, emotion_scores.rolling(7).mean()], axis=1).rename({0: 'date'}, axis=1)\n",
    "emotion_scores = emotion_scores.melt('date', var_name='emotion', value_name='score')\n",
    "# melt emotions for easier plotting if read in wide form\n",
    "emotion_scores.to_json('clean_data/emotion_scores_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ancient-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2016-12-31\n",
       "1      2017-01-01\n",
       "2      2017-01-02\n",
       "3      2017-01-03\n",
       "4      2017-01-04\n",
       "          ...    \n",
       "8715   2020-12-22\n",
       "8716   2020-12-24\n",
       "8717   2020-12-26\n",
       "8718   2020-12-28\n",
       "8719   2020-12-30\n",
       "Name: date, Length: 8720, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_scores = pd.read_json('clean_data/emotion_scores_long.json')\n",
    "emotion_scores.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "racial-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable('json')\n",
    "\n",
    "selection = alt.selection_multi(fields=['emotion'], bind='legend')\n",
    "series = alt.Chart(emotion_scores).mark_line(\n",
    "                                  ).encode(x='date:T', y='score:Q', color='emotion:N',\n",
    "                                           opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "                                           tooltip=['score']\n",
    "                                  ).properties(title='Average Emotional Tokens per Post', width=800, height=300\n",
    "                                  ).add_selection(selection\n",
    "                                  ).interactive(bind_y=False)\n",
    "\n",
    "alt.data_transformers.enable('default')\n",
    "series.save('figures/emotion_freq.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generating word clouds for each of the weeks\n",
    "# for i in range(0,24):\n",
    "#     titles = data[i]['title']\n",
    "#     texts = data[i]['text']\n",
    "#     list_of_words = ''\n",
    "#     for j in titles:\n",
    "#         tokens = nltk.word_tokenize(j.lower())\n",
    "#         list_of_words += \" \".join(tokens)+\" \"\n",
    "#     for k in texts:\n",
    "#         tokens = nltk.word_tokenize(k.lower())\n",
    "#         list_of_words += \" \".join(tokens)+\" \"\n",
    "#     wc = WordCloud().generate(list_of_words)   \n",
    "#     plt.imshow(wc)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-separate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
