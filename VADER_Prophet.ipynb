{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from fbprophet import Prophet\n",
    "import plotly\n",
    "from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enabling-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to be whatever subreddit you want to analyze\n",
    "subreddit = 'depression_help'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-marsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't even know where to begin. I've never ...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earlier this year I lost a pillar in my life t...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am so filled with anger and rage I can't kee...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I drink, to excess, then I get sad. Really sad...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My closest childhood friend has had a rough se...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       time score\n",
       "0   I don't even know where to begin. I've never ...  01Jan2017     7\n",
       "1  Earlier this year I lost a pillar in my life t...  01Jan2017     2\n",
       "2  I am so filled with anger and rage I can't kee...  01Jan2017     3\n",
       "3  I drink, to excess, then I get sad. Really sad...  01Jan2017     2\n",
       "4  My closest childhood friend has had a rough se...  01Jan2017     3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Change when changing subreddit\n",
    "df = pd.read_csv(f'{subreddit}_data/raw_data_2017/01Jan2017.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "for year in range(2017,2021):\n",
    "    #TODO: Change when changing subreddit\n",
    "    for file in os.listdir(f'{subreddit}_data/raw_data_{year}'):\n",
    "        if file[-3:] == 'csv':\n",
    "            #TODO: Change when changing subreddit\n",
    "            df_new = pd.read_csv(f'{subreddit}_data/raw_data_{year}/{file}').drop(['Unnamed: 0'], axis=1)\n",
    "            df_new.time = df_new.time.apply(lambda x: file[:-4])\n",
    "            df = pd.concat([df, df_new]).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "golden-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time = df.time.apply(lambda x: datetime.strptime(x, '%d%b%Y'))\n",
    "df = df.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constitutional-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted([datetime.strftime(datetime.strptime(file[:-4], '%d%b%Y'), '%Y-%m-%d') \\\n",
    "                for year in range(2017, 2021) \\\n",
    "                for file in os.listdir(f'{subreddit}_data/raw_data_{year}') \\\n",
    "                if file[-3:] == 'csv'])\n",
    "data = []\n",
    "df_text = df[df['text'] != '[deleted]']\n",
    "\n",
    "for date in dates:\n",
    "    data.append(df_text[df_text['time'] == datetime.strptime(date, '%Y-%m-%d')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "primary-christopher",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 692/692 [02:44<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#prophet expects a dataframe with (date, value)\n",
    "prophet_input = pd.DataFrame(columns = ['ds','y'])\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in tqdm(range(0, len(data))):\n",
    "    texts = data[i]['text'].values\n",
    "    if (len(texts) == 0): \n",
    "        continue\n",
    "    date = data[i]['time'].iloc[0] #date will be the same for all values in list, just pick first\n",
    "    #iterate over the posts in the texts list\n",
    "    for j in texts:\n",
    "        try:\n",
    "            vs = analyzer.polarity_scores(j)\n",
    "            new_data = pd.DataFrame([[date, vs['compound']]], columns = ['ds','y'])\n",
    "            prophet_input = pd.concat([prophet_input, new_data], ignore_index = True)\n",
    "        except:\n",
    "            print(\"We couldn't process this post because it was: \", j)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "balanced-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to split dataframe that was created above into four chunks (based on year) and then groupby date\n",
    "data_2017 = prophet_input[prophet_input['ds'].dt.year == 2017] \n",
    "data_2018 = prophet_input[prophet_input['ds'].dt.year == 2018] \n",
    "data_2019 = prophet_input[prophet_input['ds'].dt.year == 2019] \n",
    "data_2020 = prophet_input[prophet_input['ds'].dt.year == 2020] \n",
    "grouped_2017 = data_2017.groupby([data_2017['ds'].dt.date]).mean()\n",
    "grouped_2018 = data_2018.groupby([data_2018['ds'].dt.date]).mean()\n",
    "grouped_2019 = data_2019.groupby([data_2019['ds'].dt.date]).mean()\n",
    "grouped_2020 = data_2020.groupby([data_2020['ds'].dt.date]).mean()\n",
    "df_2017 = grouped_2017.reset_index()\n",
    "df_2018 = grouped_2018.reset_index()\n",
    "df_2019 = grouped_2019.reset_index()\n",
    "df_2020 = grouped_2020.reset_index()\n",
    "df_all_years = pd.concat([df_2017,df_2018,df_2019,df_2020])\n",
    "df_all_years.set_index('ds', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "standard-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df_all_years.reset_index().rename(columns ={'ds': 'date', 'y':'Average Vader Score'})\n",
    "#get rolling average of all data and plot it overtime\n",
    "smoothed_vader_score = df_all_years.rolling(7).mean().reset_index().rename(columns ={'ds': 'date', 'y':'Average Vader Score'})\n",
    "#convert to datetime because altair was being weird\n",
    "freq[\"date\"] = pd.to_datetime(freq[\"date\"])\n",
    "smoothed_vader_score[\"date\"] = pd.to_datetime(smoothed_vader_score[\"date\"])\n",
    "\n",
    "series = alt.Chart(freq).mark_line(opacity=0.3,\n",
    "                                  ).encode(x='date', y='Average Vader Score'\n",
    "                                  ).properties(title=f'VADER compound score of post on r/{subreddit}' \n",
    "                                  ).interactive(bind_y=False)\n",
    "\n",
    "smoothed = alt.Chart(smoothed_vader_score.reset_index()).mark_line(\n",
    "                                                           ).encode(x='date', y='Average Vader Score'\n",
    "                                                           ).properties(title=f'Vader score of r/{subreddit} Posts' \n",
    "                                                           ).interactive(bind_y=False)\n",
    "\n",
    "chart = (series + smoothed).properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/VADER_Score.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lyric-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  Average VADER Score\n",
      "0    2017-01-01             1.000000\n",
      "1    2017-01-03             0.750000\n",
      "2    2017-01-05             0.600000\n",
      "3    2017-01-07             0.666667\n",
      "4    2017-01-09             0.666667\n",
      "..          ...                  ...\n",
      "685  2020-12-22             0.636364\n",
      "686  2020-12-24             0.472222\n",
      "687  2020-12-26             0.585366\n",
      "688  2020-12-28             0.521739\n",
      "689  2020-12-30             0.702703\n",
      "\n",
      "[690 rows x 2 columns]\n",
      "     Average VADER Score\n",
      "0                    NaN\n",
      "1                    NaN\n",
      "2                    NaN\n",
      "3                    NaN\n",
      "4                    NaN\n",
      "..                   ...\n",
      "685             0.590084\n",
      "686             0.570243\n",
      "687             0.564071\n",
      "688             0.568837\n",
      "689             0.593827\n",
      "\n",
      "[690 rows x 1 columns]\n",
      "           date                                     smoothed_score\n",
      "0    2017-01-01  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "1    2017-01-03  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "2    2017-01-05  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "3    2017-01-07  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "4    2017-01-09  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "..          ...                                                ...\n",
      "685  2020-12-22  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "686  2020-12-24  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "687  2020-12-26  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "688  2020-12-28  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "689  2020-12-30  (A, v, e, r, a, g, e,  , V, A, D, E, R,  , S, ...\n",
      "\n",
      "[690 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type date is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e6602607b9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mVADER_Norm_Posts_Rolling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_smoothed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbind_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVADER_Norm_Posts_Rolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'figures/{subreddit}/Frequency_of_low_VADER_norm_posts.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, override_data_transformer, scale_factor, vegalite_version, vega_version, vegaembed_version, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverride_data_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdata_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_max_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/utils/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(chart, fp, vega_version, vegaembed_version, format, mode, vegalite_version, embed_options, json_kwds, webdriver, scale_factor, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUndefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moriginal_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUndefined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(data, context)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# consolidate inline data to top-level datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate_datasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_consolidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# if data is still not a recognized type, then return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36m_consolidate_data\u001b[0;34m(data, context)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUndefined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dataset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36m_dataset_name\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInlineDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mvalues_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mhsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"data-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhsh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type date is not JSON serializable"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat([data_2017,data_2018,data_2019,data_2020])\n",
    "df_low_VADER = df_all[df_all['y'] < -0.05]\n",
    "posts_per_day_low_VADER = df_low_VADER.groupby([df_low_VADER['ds'].dt.date]).count()\n",
    "posts_per_day = df_all.groupby([df_all['ds'].dt.date]).count()\n",
    "normalized_VADER_count = (posts_per_day_low_VADER/posts_per_day).y\n",
    "\n",
    "a = normalized_VADER_count.index.to_series()\n",
    "\n",
    "df = pd.DataFrame({'date': a, 'Average VADER Score': normalized_VADER_count}).reset_index().drop(columns=['ds'])\n",
    "print(df)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "smoothed_VADER_Frequency_Score = df.rolling(7).mean()\n",
    "print(smoothed_VADER_Frequency_Score)\n",
    "df_smoothed = pd.DataFrame({'date': a, 'smoothed_score': smoothed_VADER_Frequency_Score}).reset_index().drop(columns=['ds'])\n",
    "print(df_smoothed)\n",
    "VADER_Norm_Posts = alt.Chart(df).interactive(bind_y=False).mark_line(opacity=0.3).encode(x='date',y='count').properties(title=f'Frequency of posts with compound score less than -0.05 on r/{subreddit} normalized by number of posts (on that day)')\n",
    "VADER_Norm_Posts_Rolling = alt.Chart(df_smoothed).mark_line(opacity=0.1).interactive(bind_y=False).encode(x='date',y='count')\n",
    "chart = (VADER_Norm_Posts_Rolling).properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/Frequency_of_low_VADER_norm_posts.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "modern-baseline",
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaValidationError",
     "evalue": "Invalid specification\n\n        altair.vegalite.v4.api.Chart, validating 'required'\n\n        'mark' is a required property\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaValidationError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ca91d88b869a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mVADER_Norm_Subs_rolling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new_subs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbind_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVADER_Norm_Subs_rolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'figures/{subreddit}/Frequency_of_low_VADER_norm_subs.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, override_data_transformer, scale_factor, vegalite_version, vega_version, vegaembed_version, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverride_data_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdata_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_max_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/utils/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(chart, fp, vega_version, vegaembed_version, format, mode, vegalite_version, embed_options, json_kwds, webdriver, scale_factor, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/vegalite/v4/api.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validate\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deep\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTopLevelMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# TODO: following entries are added after validation. Should they be validated?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/altair/utils/schemapi.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, validate, ignore, context)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSchemaValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSchemaValidationError\u001b[0m: Invalid specification\n\n        altair.vegalite.v4.api.Chart, validating 'required'\n\n        'mark' is a required property\n        "
     ]
    }
   ],
   "source": [
    "trend = pd.read_csv(f'{subreddit}_data/subscriber_count.csv')\n",
    "trend['subscribers'] = trend['subscribers'].interpolate()\n",
    "trend['date'] = trend['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').replace(hour=0, minute=0))\n",
    "overlap = pd.DataFrame()\n",
    "dates = freq[freq['date'].isin(trend.date)]['date']\n",
    "overlap['date'] = dates\n",
    "overlap_subs = trend[trend['date'].isin(dates)].reset_index().drop(['index'], axis=1)\n",
    "overlap_subs_new = overlap_subs.set_index('date')\n",
    "normalized_VADER_subscribers = (posts_per_day_low_VADER['y']/overlap_subs_new['subscribers'])\n",
    "df_subs = pd.DataFrame({'date': a, 'count':normalized_VADER_subscribers}).reset_index().drop(columns=['index'])\n",
    "df_subs['date'] = pd.to_datetime(df['date'])\n",
    "df_new_subs = df_subs\n",
    "df_new_subs['count'] = df_new_subs['count'].rolling(7).mean()\n",
    "#VADER_Norm_Subs = alt.Chart(df_subs).interactive(bind_y=False).mark_line(opacity=0.3).encode(x='date', y='count').properties(title=f'Frequency of posts with compound score less than -0.05 on r/{subreddit} normalized by number of subscribers (on that day)')\n",
    "VADER_Norm_Subs_rolling = alt.Chart(df_new_subs).interactive(bind_y=False).encode(x='date', y='count')\n",
    "chart = (VADER_Norm_Subs_rolling).properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/Frequency_of_low_VADER_norm_subs.html')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-gates",
   "metadata": {},
   "source": [
    "## Everything below this line is for predicting with Prophet, TBD if this will still even be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#periodgram of data\n",
    "f, Pxx_den = signal.periodogram(df_all_years['y'])\n",
    "plt.semilogy(f, Pxx_den)\n",
    "plt.ylim([1e-7, 1e2])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start fitting data using Prophet model\n",
    "m_2017 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2017 data, use the to predict 2018 data\n",
    "m_2017.fit(df_2017)\n",
    "future_2018 = m_2017.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2018 = m_2017.predict(future_2018)\n",
    "y_hat_2018 = forecast_2018[['ds','yhat']]\n",
    "fig1 = m_2017.plot(forecast_2018)\n",
    "#m_2017.plot_components(forecast_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted_2018 vs true_2018 to check for similarities\n",
    "plt.plot(y_hat_2018['ds'], y_hat_2018['yhat'])\n",
    "plt.plot(df_2018['ds'],df_2018['y'])\n",
    "#find correlation between the two values as well\n",
    "y_hat_2018_trim = y_hat_2018.iloc[::2]\n",
    "print(np.corrcoef(y_hat_2018_trim['yhat'], df_2018['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2018 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2018 data, use the to predict 2019 data\n",
    "m_2018.fit(df_2018)\n",
    "future_2019 = m_2018.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2019 = m_2018.predict(future_2019)\n",
    "y_hat_2019 = forecast_2019[['ds','yhat']]\n",
    "y_hat_2019 = y_hat_2019.iloc[::2]\n",
    "fig2 = m_2018.plot(forecast_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted_2019 vs true_2019 to check for similarities\n",
    "plt.plot(y_hat_2019['ds'], y_hat_2019['yhat'])\n",
    "plt.plot(df_2019['ds'],df_2019['y'])\n",
    "#find the correlation between y_hat and y_true\n",
    "y_hat_2019_trim = y_hat_2019.iloc[::2]\n",
    "np.corrcoef(y_hat_2019['yhat'], df_2019['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2019 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2018 data, use the to predict 2019 data\n",
    "m_2019.fit(df_2019)\n",
    "future_2020 = m_2019.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2020 = m_2019.predict(future_2020)\n",
    "y_hat_2020 = forecast_2020[['ds','yhat']]\n",
    "fig3 = m_2019.plot(forecast_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat_2020['ds'], y_hat_2020['yhat'])\n",
    "plt.plot(df_2020['ds'],df_2020['y'])\n",
    "#find the correlation between y_hat and y_true\n",
    "y_hat_2020_trim = y_hat_2020.iloc[::2]\n",
    "np.corrcoef(y_hat_2020_trim['yhat'], df_2020['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try fitting 2017-2019 to predit 2020\n",
    "df_pre2020 = pd.concat([df_2017,df_2018,df_2019])\n",
    "m_2020 = Prophet(weekly_seasonality=False, yearly_seasonality=True)\n",
    "m_2020.fit(df_pre2020)\n",
    "future_all = m_2020.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2020_all = m_2020.predict(future_all)\n",
    "y_hat_2020_all = forecast_2020_all[['ds','yhat']]\n",
    "fig4 = m_2020.plot(forecast_2020_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat_2020_all['ds'], y_hat_2020_all['yhat'])\n",
    "plt.plot(df_2020['ds'],df_2020['y'])\n",
    "#find correlation between true and predicted value\n",
    "y_hat_2020_all_trim = y_hat_2020_all.iloc[::2]\n",
    "np.corrcoef(y_hat_2020_all_trim['yhat'], df_2020['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check for correlation between these two years\n",
    "#TODO: investigate changepoints of the prophet model\n",
    "#TODO: Try to smooth initial data more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-finding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
