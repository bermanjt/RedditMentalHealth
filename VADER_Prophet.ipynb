{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from fbprophet import Prophet\n",
    "import plotly\n",
    "from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "russian-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to be whatever subreddit you want to analyze\n",
    "subreddit = 'MentalHealth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-marsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't know how many of you read it, but a fe...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The reason that it occurred to me that I may h...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sometimes I don't really feel like a participa...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi, and thanks for reading this post. I'm look...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Loaded question I know, trying to scribble thi...</td>\n",
       "      <td>01Jan2017</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       time  score\n",
       "0  I don't know how many of you read it, but a fe...  01Jan2017      6\n",
       "1  The reason that it occurred to me that I may h...  01Jan2017      1\n",
       "2  Sometimes I don't really feel like a participa...  01Jan2017      5\n",
       "3  Hi, and thanks for reading this post. I'm look...  01Jan2017      1\n",
       "4  Loaded question I know, trying to scribble thi...  01Jan2017      7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Change when changing subreddit\n",
    "df = pd.read_csv(f'{subreddit}_data/raw_data_2017/01Jan2017.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "for year in range(2017,2021):\n",
    "    #TODO: Change when changing subreddit\n",
    "    for file in os.listdir(f'{subreddit}_data/raw_data_{year}'):\n",
    "        if file[-3:] == 'csv':\n",
    "            #TODO: Change when changing subreddit\n",
    "            df_new = pd.read_csv(f'{subreddit}_data/raw_data_{year}/{file}').drop(['Unnamed: 0'], axis=1)\n",
    "            df_new.time = df_new.time.apply(lambda x: file[:-4])\n",
    "            df = pd.concat([df, df_new]).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "golden-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time = df.time.apply(lambda x: datetime.strptime(x, '%d%b%Y'))\n",
    "df = df.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constitutional-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted([datetime.strftime(datetime.strptime(file[:-4], '%d%b%Y'), '%Y-%m-%d') \\\n",
    "                for year in range(2017, 2021) \\\n",
    "                for file in os.listdir(f'{subreddit}_data/raw_data_{year}') \\\n",
    "                if file[-3:] == 'csv'])\n",
    "data = []\n",
    "df_text = df[df['text'] != '[deleted]']\n",
    "\n",
    "for date in dates:\n",
    "    data.append(df_text[df_text['time'] == datetime.strptime(date, '%Y-%m-%d')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "primary-christopher",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 732/732 [17:56<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "#prophet expects a dataframe with (date, value)\n",
    "prophet_input = pd.DataFrame(columns = ['ds','y'])\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in tqdm(range(0, len(data))):\n",
    "    texts = data[i]['text'].values\n",
    "    if (len(texts) == 0): \n",
    "        continue\n",
    "    date = data[i]['time'].iloc[0] #date will be the same for all values in list, just pick first\n",
    "    #iterate over the posts in the texts list\n",
    "    for j in texts:\n",
    "        try:\n",
    "            vs = analyzer.polarity_scores(j)\n",
    "            new_data = pd.DataFrame([[date, vs['compound']]], columns = ['ds','y'])\n",
    "            prophet_input = pd.concat([prophet_input, new_data], ignore_index = True)\n",
    "        except:\n",
    "            print(\"We couldn't process this post because it was: \", j)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "balanced-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to split dataframe that was created above into four chunks (based on year) and then groupby date\n",
    "data_2017 = prophet_input[prophet_input['ds'].dt.year == 2017] \n",
    "data_2018 = prophet_input[prophet_input['ds'].dt.year == 2018] \n",
    "data_2019 = prophet_input[prophet_input['ds'].dt.year == 2019] \n",
    "data_2020 = prophet_input[prophet_input['ds'].dt.year == 2020] \n",
    "grouped_2017 = data_2017.groupby([data_2017['ds'].dt.date]).mean()\n",
    "grouped_2018 = data_2018.groupby([data_2018['ds'].dt.date]).mean()\n",
    "grouped_2019 = data_2019.groupby([data_2019['ds'].dt.date]).mean()\n",
    "grouped_2020 = data_2020.groupby([data_2020['ds'].dt.date]).mean()\n",
    "df_2017 = grouped_2017.reset_index()\n",
    "df_2018 = grouped_2018.reset_index()\n",
    "df_2019 = grouped_2019.reset_index()\n",
    "df_2020 = grouped_2020.reset_index()\n",
    "df_all_years = pd.concat([df_2017,df_2018,df_2019,df_2020])\n",
    "df_all_years.set_index('ds', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "standard-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df_all_years.reset_index().rename(columns ={'ds': 'date', 'y':'count'})\n",
    "#get rolling average of all data and plot it overtime\n",
    "smoothed_vader_score = df_all_years.rolling(7).mean().reset_index().rename(columns ={'ds': 'date', 'y':'count'})\n",
    "#convert to datetime because altair was being weird\n",
    "freq[\"date\"] = pd.to_datetime(freq[\"date\"])\n",
    "smoothed_vader_score[\"date\"] = pd.to_datetime(smoothed_vader_score[\"date\"])\n",
    "\n",
    "series = alt.Chart(freq).mark_line(opacity=0.3,\n",
    "                                  ).encode(x='date', y='count'\n",
    "                                  ).properties(title=f'VADER compound score of post on r/{subreddit}' \n",
    "                                  ).interactive(bind_y=False)\n",
    "\n",
    "smoothed = alt.Chart(smoothed_vader_score.reset_index()).mark_line(\n",
    "                                                           ).encode(x='date', y='count'\n",
    "                                                           ).properties(title=f'Vader score of r/{subreddit} Posts' \n",
    "                                                           ).interactive(bind_y=False)\n",
    "#TODO: Change when changing subreddit\n",
    "chart = (series + smoothed).properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/VADER_Score.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lyric-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([data_2017,data_2018,data_2019,data_2020])\n",
    "df_low_VADER = df_all[df_all['y'] < -0.05]\n",
    "posts_per_day_low_VADER = df_low_VADER.groupby([df_low_VADER['ds'].dt.date]).count()\n",
    "posts_per_day = df_all.groupby([df_all['ds'].dt.date]).count()\n",
    "normalized_VADER_count = (posts_per_day_low_VADER/posts_per_day).y\n",
    "a = normalized_VADER_count.index.to_series()\n",
    "\n",
    "df = pd.DataFrame({'date': a, 'count': normalized_VADER_count}).reset_index().drop(columns=['ds'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "VADER_Norm_Posts = alt.Chart(df).interactive(bind_y=False).mark_line(opacity=0.3).encode(x='date',y='count').properties(title=f'Frequency of posts with compound score less than -0.05 on r/{subreddit} normalized by number of posts (on that day)')\n",
    "chart = VADER_Norm_Posts.properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/Frequency_of_low_VADER_norm_posts.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modern-baseline",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '1/1/2017 18:00' does not match format '%Y-%m-%d %H:%M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3ae122eebeec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{subreddit}_data/subscriber_count.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subscribers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subscribers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3ae122eebeec>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{subreddit}_data/subscriber_count.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subscribers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subscribers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EECS_Affective/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 359\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '1/1/2017 18:00' does not match format '%Y-%m-%d %H:%M'"
     ]
    }
   ],
   "source": [
    "trend = pd.read_csv(f'{subreddit}_data/subscriber_count.csv')\n",
    "trend['subscribers'] = trend['subscribers'].interpolate()\n",
    "trend['date'] = trend['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').replace(hour=0, minute=0))\n",
    "overlap = pd.DataFrame()\n",
    "dates = freq[freq['date'].isin(trend.date)]['date']\n",
    "overlap['date'] = dates\n",
    "overlap_subs = trend[trend['date'].isin(dates)].reset_index().drop(['index'], axis=1)\n",
    "overlap_subs_new = overlap_subs.set_index('date')\n",
    "normalized_VADER_subscribers = (posts_per_day_low_VADER['y']/overlap_subs_new['subscribers'])\n",
    "df_subs = pd.DataFrame({'date': a, 'count':normalized_VADER_subscribers}).reset_index().drop(columns=['index'])\n",
    "df_subs['date'] = pd.to_datetime(df['date'])\n",
    "VADER_Norm_Subs = alt.Chart(df_subs).interactive(bind_y=False).mark_line(opacity=0.3).encode(x='date', y='count').properties(title=f'Frequency of posts with compound score less than -0.05 on r/{subreddit} normalized by number of subscribers (on that day)')\n",
    "chart = VADER_Norm_Subs.properties(width=800, height=300)\n",
    "chart.save(f'figures/{subreddit}/Frequency_of_low_VADER_norm_subs.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-gates",
   "metadata": {},
   "source": [
    "## Everything below this line is for predicting with Prophet, TBD if this will still even be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#periodgram of data\n",
    "f, Pxx_den = signal.periodogram(df_all_years['y'])\n",
    "plt.semilogy(f, Pxx_den)\n",
    "plt.ylim([1e-7, 1e2])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start fitting data using Prophet model\n",
    "m_2017 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2017 data, use the to predict 2018 data\n",
    "m_2017.fit(df_2017)\n",
    "future_2018 = m_2017.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2018 = m_2017.predict(future_2018)\n",
    "y_hat_2018 = forecast_2018[['ds','yhat']]\n",
    "fig1 = m_2017.plot(forecast_2018)\n",
    "#m_2017.plot_components(forecast_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted_2018 vs true_2018 to check for similarities\n",
    "plt.plot(y_hat_2018['ds'], y_hat_2018['yhat'])\n",
    "plt.plot(df_2018['ds'],df_2018['y'])\n",
    "#find correlation between the two values as well\n",
    "y_hat_2018_trim = y_hat_2018.iloc[::2]\n",
    "print(np.corrcoef(y_hat_2018_trim['yhat'], df_2018['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2018 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2018 data, use the to predict 2019 data\n",
    "m_2018.fit(df_2018)\n",
    "future_2019 = m_2018.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2019 = m_2018.predict(future_2019)\n",
    "y_hat_2019 = forecast_2019[['ds','yhat']]\n",
    "y_hat_2019 = y_hat_2019.iloc[::2]\n",
    "fig2 = m_2018.plot(forecast_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted_2019 vs true_2019 to check for similarities\n",
    "plt.plot(y_hat_2019['ds'], y_hat_2019['yhat'])\n",
    "plt.plot(df_2019['ds'],df_2019['y'])\n",
    "#find the correlation between y_hat and y_true\n",
    "y_hat_2019_trim = y_hat_2019.iloc[::2]\n",
    "np.corrcoef(y_hat_2019['yhat'], df_2019['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2019 = Prophet(weekly_seasonality=False)\n",
    "#fit model with 2018 data, use the to predict 2019 data\n",
    "m_2019.fit(df_2019)\n",
    "future_2020 = m_2019.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2020 = m_2019.predict(future_2020)\n",
    "y_hat_2020 = forecast_2020[['ds','yhat']]\n",
    "fig3 = m_2019.plot(forecast_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat_2020['ds'], y_hat_2020['yhat'])\n",
    "plt.plot(df_2020['ds'],df_2020['y'])\n",
    "#find the correlation between y_hat and y_true\n",
    "y_hat_2020_trim = y_hat_2020.iloc[::2]\n",
    "np.corrcoef(y_hat_2020_trim['yhat'], df_2020['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try fitting 2017-2019 to predit 2020\n",
    "df_pre2020 = pd.concat([df_2017,df_2018,df_2019])\n",
    "m_2020 = Prophet(weekly_seasonality=False, yearly_seasonality=True)\n",
    "m_2020.fit(df_pre2020)\n",
    "future_all = m_2020.make_future_dataframe(periods=365)[-365:]\n",
    "forecast_2020_all = m_2020.predict(future_all)\n",
    "y_hat_2020_all = forecast_2020_all[['ds','yhat']]\n",
    "fig4 = m_2020.plot(forecast_2020_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat_2020_all['ds'], y_hat_2020_all['yhat'])\n",
    "plt.plot(df_2020['ds'],df_2020['y'])\n",
    "#find correlation between true and predicted value\n",
    "y_hat_2020_all_trim = y_hat_2020_all.iloc[::2]\n",
    "np.corrcoef(y_hat_2020_all_trim['yhat'], df_2020['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check for correlation between these two years\n",
    "#TODO: investigate changepoints of the prophet model\n",
    "#TODO: Try to smooth initial data more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-finding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
